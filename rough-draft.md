ABSTRACT

Solid-state drives have entered the mainstream of commercial storage devices. These drives come equipped with their own processors and memory to provide the necessary support for the flash translation layer. Numerous solutions exist for enabling the host machine to leverage these in-storage computational resources, making possible considerable performance gains. However, these existing approaches rely heavily on hand-written code running on the SSD, making them inflexible for adaptation to new applications, and inaccessible to application developers writing in modern, high-level programming languages. This paper proposes to redress this weakness by moving the task of code generation for the SSD from the programmer to the host compiler. We target Pyston, a just-in-time Python compiler, and use built-in Python functionality to share relevant data structures and machine code directly with the SSD. [rest of abstract will deal with results]

I. INTRODUCTION [citations needed…?]

In keeping with the Von Neumann model of computer architecture, computer systems have traditionally separated computation, performed by a dedicated processor, from data storage, typically accomplished by a hard disk drive (HDD), or more recently, a solid state drive (SSD). Over the past few decades, processor performance has improved dramatically. Storage access latency has not enjoyed similar improvement.

The result of this mismatch is that storage access has become the most significant limiting factor for computing performance. This is particularly true for data intensive applications, which have become ever more prevalent with the advent of big data.

In the last few years, SSDs have entered the commercial mainstream of storage devices. SSDs offer significant performance improvement in access latency over HDDs. However, as flash-based devices, SSDs also differ from HDDs in a number of use-relevant ways. Unlike HDDs, SSDs have different granularity for reads and writes, cannot write to dirty pages before clearing them, and have lifetime limits on number of writes per block before a given block becomes unusable. These differences necessitate different access patterns from those appropriate for HDDs. To work seamlessly with the HDD-optimized storage interfaces common among host machines, SSDs must therefore perform additional logic known as the flash translation layer (FTL) to process traditional read-write requests from the host. To accomplish this, SSDs come equipped with their own internal processors and DRAM.

The additional computational resources in peripheral devices such as SSDs create an opportunity to bypass the bottleneck of data transfer by moving away from the classical von Neumann separation of computation and data storage. By moving some of the computational tasks in a program to near-data processors such as those found in SSDs, a more distributed model of computing can reduce the traffic between the central processing unit and peripheral devices.

The work that most directly inspired the current project is described in a paper titled, “Morpheus: Creating Application Objects Efficiently for Heterogeneous Computing”. Dr. Tseng et al present a model of computation called Morpheus, in which the host machine compiles and transmits code to the SSD, which then processes data and transmits the results back to a corresponding program running on the host machine, generated by the same compiler. By targeting the data-intensive task of deserialization – specifically, of ASCII stored values into C-compatible integer arrays – their Morpheus-SSD implementation reduced context switching by an average of 98% in the host machine, energy consumption by upwards of 42%, and execution time by varying but significant margins on a wide range of applications.

The primary drawback of the Morpheus project is its reliance on the programmer to manually write code intended for execution on the SSD. Code to be executed on the SSD must be written in C, and is exceptionally challenging to debug. These factors make Morpheus less flexible and programmer-friendly than is practical for adoption by mainstream application developers.

The project undertaken in this paper is to adapt the Morpheus programming model to work with a high level programming language in a manner that is transparent for application developers. To this end, we extend a just-in-time compiler for the Python programming language called Pyston.

The key components of our programming model include the running Python process, a kernel module providing a memory buffer, and the kernel on the SSD.

We target Python primarily for its high popularity among application developers. Our reasons for targeting Pyston instead of other Python implementations are severalfold. The first and most important of these is that Pyston is a JIT compiler. JIT compilers are much more conducive to our project than the line-by-line interpretation of the standard CPython implementation of Python because they provide much better support for runtime generation of machine code executables to be transmitted to the SSD.

Other JIT compiler projects for Python exist though. Like Pyston, a number of these projects, such as PyPy and Numba, are performance oriented. What makes Pyston unique among these is that it is built on top of LLVM, a well-known, highly modular, open-source compiler toolchain. This LLVM backend allows us to generate LLVM intermediate representation for the code segments that we wish to run on the SSD. Because LLVM is so modular, this LLVM IR can then be compiled into appropriate machine code for virtually any device - in our case, an ARM-based processor on the SSD, running an Ubuntu Linux kernel.

[what, if anything, should I say about the SSD we are using?]

To share compiled code and necessary data structures with the SSD, we use the built-in Python mmap module, a Python-language wrapper for the mmap system call. The running Python process uses mmap to share the necessary process memory with a dedicated kernel memory buffer. The kernel then shares this memory with the SSD using a slightly extended NVMe interface. The SSD then executes the compiled code, and writes the results back into the host-side shared buffer using DMA.

The remainder of this paper is broken down into the following sections:
II. Background
III. Detailed system overview
IV. Generation of machine code from Pyston
V. Results
VI. Related work
VII. Conclusion

II. Background [not ready]

III - V [not ready]

VI. Related work

Researchers have taken several approaches in recent years to the possibility of exploiting underutilized in-storage processors. These include Summarizer, which allows host applications to offload work to the SSD via extensions of the NVMe interface, and Biscuit, a computation model focused on data flow which avoids distinguishing between host-side and storage-side processes. [comment on performance benefits relative to ours depends on our results]

Other strategies call on the in-storage processor to perform OS-level work, rather than offloading user application computations. This is exemplified by the Slacker scheduler. This paper observes that sub-requests on an SSD are often scheduled on different flash chips and thus often complete at different times, the gaps between which they call “slack”. The Slacker scheduler uses the in-storage processor to optimize the ordering of sub-requests on different flash chips to improve overall response times for requests to the SSD.

Other relevant work relates to changing the balance of existing work between the host and the SSD.

The paper “Application Managed Flash” proposes moving much of the FTL logic up to the host system by employing an append-only log-based file system. This would free up the in-storage processor to take on a significantly greater computational load from the host processor.

DIDACache focuses on key-value cache systems, and seeks to give the host more direct control over the storage of data on the SSD. Similar to Application Managed Flash, this makes SSD resources more available to the host, and helps further incorporate in-storage resources and structure with the host system.


VII. Conclusion [not ready]
